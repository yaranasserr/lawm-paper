<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title> Latent Action Pretraining Through World Modeling (LAWM) </title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f8f9fa;
        }

        .header {
            background: linear-gradient(135deg, #4b7fb3 0%, #71a1d5 100%);
            color: white;
            padding: 60px 20px;
            text-align: center;
        }

        .header h1 {
            font-size: 2.5em;
            margin-bottom: 20px;
            font-weight: 700;
        }

        .authors {
            font-size: 1.2em;
            margin-bottom: 15px;
        }

        .affiliation {
            font-size: 1em;
            opacity: 0.9;
            margin-bottom: 30px;
        }

        .links {
            display: flex;
            gap: 15px;
            justify-content: center;
            flex-wrap: wrap;
        }

        .btn {
            padding: 12px 24px;
            background: white;
            color: #4b7fb3;
            text-decoration: none;
            border-radius: 8px;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            display: inline-block;
        }

        .btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.2);
        }

        .container {
            max-width: 1100px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        .section {
            background: white;
            border-radius: 12px;
            padding: 40px;
            margin-bottom: 30px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .section h2 {
            font-size: 2em;
            margin-bottom: 20px;
            color: #4b7fb3;
            border-bottom: 3px solid #71a1d5;
            padding-bottom: 10px;
        }

        .section h3 {
            font-size: 1.5em;
            margin-top: 25px;
            margin-bottom: 15px;
            color: #555;
        }

        .abstract {
            font-size: 1.1em;
            line-height: 1.8;
            text-align: justify;
        }

        .video-container {
            position: relative;
            padding-bottom: 56.25%;
            height: 0;
            overflow: hidden;
            border-radius: 8px;
            margin: 20px 0;
        }

        .video-container iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }

        .placeholder-img {
            width: 100%;
            background: linear-gradient(135deg, #71a1d520 0%, #4b7fb320 100%);
            border-radius: 8px;
            padding: 60px 20px;
            text-align: center;
            color: #4b7fb3;
            font-size: 1.2em;
            margin: 20px 0;
        }

        .grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .card {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid #dc5969;
        }

        .card h4 {
            color: #b23b4d;
            margin-bottom: 10px;
        }

        .citation-box {
            background: #f8f9fa;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 20px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            overflow-x: auto;
            margin: 20px 0;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        .comparison-table th,
        .comparison-table td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }

        .comparison-table th {
            background: #4b7fb3;
            color: white;
            font-weight: 600;
        }

        .comparison-table tr:hover {
            background: #f8f9fa;
        }

        footer {
            text-align: center;
            padding: 30px 20px;
            color: #666;
            background: white;
            margin-top: 40px;
        }

        @media (max-width: 768px) {
            .header h1 {
                font-size: 1.8em;
            }
            
            .section {
                padding: 25px;
            }
            
            .links {
                flex-direction: column;
                align-items: center;
            }
        }
    </style>
</head>
<body>
    <div class="header">
  <h1>Latent Action Pretraining Through World Modeling (LAWM) </h1>
  <div class="authors">
    <a href="https://www.linkedin.com/in/baheytharwat/" target="_blank">Bahey Tharwat</a><sup>1</sup>, 
    <a href="https://www.linkedin.com/in/yaranasser/" target="_blank">Yara Nasser</a><sup>2</sup>, 
    <a href="https://www.linkedin.com/in/alisharei/" target="_blank">Ali Abouzied</a><sup>1</sup>, 
    <a href="https://scholar.google.com/citations?user=ATkNLcQAAAAJ&hl=en" target="_blank">Ian Reid</a><sup>1</sup>
  </div>
  <div class="affiliation">
    <sup>1</sup> Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE<br>
    <sup>2</sup> Alexandria University, Alexandria, Egypt
  </div>
</div>




        <div class="links">
            <a href="https://arxiv.org/abs/2509.18428" class="btn">ðŸ“„ Paper</a>
            <a href="https://arxiv.org/pdf/2509.18428" class="btn">ðŸ“‘ PDF</a>
            <a href="https://github.com/baheytharwat/lawm" class="btn">ðŸ’» Code</a>
            <!-- <a href="#citation" class="btn">ðŸ“‹ Cite</a> -->
        </div>
    </div>

    <div class="container">
        <!-- Abstract Section -->
        <div class="section">
            <h2>Abstract</h2>
        <p class="abstract">
  Vision-Language-Action (VLA) models enable robots to follow language instructions but often require large labeled datasets and heavy models, limiting real-world use. Recent methods leverage latent action representations for pretraining on unlabeled videos, yet remain resource-intensive. We propose LAWM, a model-agnostic framework that learns latent actions through world modeling from unlabeled robot or human videos. LAWM transfers effectively across tasks and embodiments, outperforming ground-truth and prior pretraining methods on LIBERO and real-world experiments, while being more efficient and practical.
</p>

        </div>

        <!-- Video Section -->
        <div class="section">

  <div class="video-container">
    <video controls width="100%">
      <source src="assets/videos/lawm.mp4" type="video/mp4">
   
    </video>
  </div>
</div>


        <!-- Key Contributions -->
        <div class="section">
            <h2>Key Contributions</h2>
            <div class="grid">
                <div class="card">
                    <h4>ðŸŽ¯ Model-Agnostic Framework</h4>
                    <p>LAWM can be applied to various imitation learning architectures, making it flexible and widely applicable.</p>
                </div>
                <div class="card">
                    <h4>ðŸŽ¥ Unlabeled Video Pretraining</h4>
                    <p>Learn from unlabeled robot or human videos without expensive teleoperation data collection.</p>
                </div>
                <div class="card">
                    <h4>ðŸ”„ Cross-Embodiment Transfer</h4>
                    <p>Effective transfer across different tasks, environments, and robot embodiments.</p>
                </div>
                <div class="card">
                    <h4>âš¡ Efficient & Practical</h4>
                    <p>Smaller model size and better performance compared to large-scale VLA models.</p>
                </div>
            </div>
        </div>

        <!-- Method Overview -->
        <div class="section">
            <h2>Method Overview</h2>
            <div class="placeholder-img">
  <img src="assets/images/framework2_page-0001.jpg" alt="Method Overview" width="100%">
</div>

            <h3>Latent Action Learning</h3>
            <p>LAWM learns latent action representations through world modeling, predicting visual changes between frames without requiring ground-truth action labels. This enables pretraining on diverse unlabeled video datasets.</p>
            
            <h3>Two-Stage Training</h3>
            <p><strong>Stage 1: Self-Supervised Pretraining</strong> - Learn latent actions from unlabeled videos through world modeling.</p>
            <p><strong>Stage 2: Task-Specific Finetuning</strong> - Finetune on target robotic manipulation tasks with minimal labeled data.</p>
        </div>

        <!-- Results -->
        <div class="section">
            <h2>Results</h2>
<h3>LIBERO Benchmark</h3>
<div class="results-container">
  <img src="assets/images/single_bar_chart_custom_narrows_baku-min.png" 
       alt="BAKU Results" style="width:48%; margin-right:2%;">
  <img src="assets/images/single_bar_chart_custom_narrows_dp-min.png" 
       alt="DP Results" style="width:48%;">
</div>

            
   <table class="comparison-table">
  <thead>
    <tr>
      <th>Model</th>
      <th>Pretraining Method</th>
      <th>Spatial</th>
      <th>Object</th>
      <th>Goal</th>
      <th>Long</th>
      <th>Average</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Octo-base</td>
      <td>Robot Actions</td>
      <td>78.90</td>
      <td>85.70</td>
      <td>84.60</td>
      <td>51.10</td>
      <td>75.10</td>
    </tr>
    <tr>
      <td>OpenVLA </td>
      <td>Robot Actions</td>
      <td>84.70</td>
      <td>88.40</td>
      <td>79.20</td>
      <td>53.70</td>
      <td>76.50</td>
    </tr>
    <tr>
      <td>&pi;<sub>0</sub> </td>
      <td>Robot Actions</td>
      <td>90.00</td>
      <td>86.00</td>
      <td>95.00</td>
      <td>73.00</td>
      <td>86.00</td>
    </tr>
    <tr>
      <td>&pi;<sub>0.5</sub> </td>
      <td>Robot Actions</td>
      <td><u>98.80</u></td>
      <td><u>98.20</u></td>
      <td><strong>98.00</strong></td>
      <td><u>92.40</u></td>
      <td><u>96.85</u></td>
    </tr>
    <tr>
      <td>&pi;<sub>0</sub> (Paligemma-3B)</td>
      <td>VLM Checkpoint</td>
      <td>87.00</td>
      <td>63.00</td>
      <td>89.00</td>
      <td>48.00</td>
      <td>71.80</td>
    </tr>
    <tr>
      <td>SmolVLA (SmolVLM-2.25B) </td>
      <td>VLM Checkpoint</td>
      <td>93.00</td>
      <td>94.00</td>
      <td>91.00</td>
      <td>77.00</td>
      <td>88.75</td>
    </tr>
    <tr>
      <td>villa-X w/o latent</td>
      <td>Videos</td>
      <td>86.00</td>
      <td>86.50</td>
      <td>85.00</td>
      <td>70.00</td>
      <td>81.90</td>
    </tr>
    <tr>
      <td>villa-X </td>
      <td>Videos</td>
      <td>97.50</td>
      <td>97.00</td>
      <td>91.50</td>
      <td>74.50</td>
      <td>90.10</td>
    </tr>
    <tr>
      <td>BAKU w/o pretraining </td>
      <td>None</td>
      <td>94.00</td>
      <td><strong>100.00</strong></td>
      <td><u>96.00</u></td>
      <td>92.00</td>
      <td>95.50</td>
    </tr>
    <tr>
      <td>BAKU w/ latent pretraining (<strong>Ours</strong>)</td>
      <td>Videos</td>
      <td><strong>99.00</strong></td>
      <td><strong>100.00</strong></td>
      <td><u>96.00</u></td>
      <td><strong>94.00</strong></td>
      <td><strong>97.25</strong></td>
    </tr>
  </tbody>
</table>

            </table>
<h3>Real-World Experiments</h3>
<div class="realworld-img">
  <img src="assets/images/realman_test_page-0001.jpg" 
       alt="Real-World Experiment Results" 
       width="100%">
</div>

        </div>


<!-- Citation -->
<div class="section" id="citation">
  <h2>Citation</h2>
  <p><strong>Subjects:</strong> Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><strong>Cite as:</strong> arXiv:2509.18428 [cs.RO] 
    (or arXiv:2509.18428v1 [cs.RO] for this version)</p>
  <p><a href="https://doi.org/10.48550/arXiv.2509.18428" target="_blank">https://doi.org/10.48550/arXiv.2509.18428</a></p>

  <p>If you find our work useful, please consider citing:</p>
  <div class="citation-box">
<pre>
@article{lawm2025,
  title   = {Latent Action Pretraining Through World Modeling},
  author  = {Bahey Tharwat and Yara Nasser and Ali Abouzied and Ian Reid},
  journal = {arXiv preprint arXiv:2509.18428},
  year    = {2025}
}
</pre>
  </div>
</div>

